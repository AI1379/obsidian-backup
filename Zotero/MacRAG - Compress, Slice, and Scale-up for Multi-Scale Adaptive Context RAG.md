# MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG

## ğŸ“’ å¼•ç”¨

```plaintext
Lim, Woosang, Zekun Li, Gyuwan Kim, et al. â€œMacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG.â€ arXiv:2505.06569. Version 2. Preprint, arXiv, May 20, 2025. [https://doi.org/10.48550/arXiv.2505.06569](https://doi.org/10.48550/arXiv.2505.06569).
```

Cite Key:

```plaintext
limMacRAGCompressSlice2025
```

URLï¼š[http://arxiv.org/abs/2505.06569](http://arxiv.org/abs/2505.06569)

## ğŸ“Œ æ‘˜è¦

Long-context large language models (LC LLMs) combined with retrieval-augmented generation (RAG) hold strong potential for complex multi-hop and large-document tasks. However, existing RAG systems often suffer from imprecise retrieval, incomplete context coverage under constrained windows, and fragmented information from suboptimal context construction. We introduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical RAG framework that compresses and partitions documents into coarse-to-fine granularities, then adaptively merges relevant contexts through real-time chunk- and document-level expansions. By initiating with finest-level retrieval and progressively incorporating broader, higher-level context, MacRAG constructs effective query-specific long contexts, optimizing both precision and coverage. Evaluations on challenging LongBench expansions of HotpotQA, 2WikiMultihopQA, and Musique confirm MacRAG consistently surpasses baseline RAG pipelines in single- and multi-step generation using Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o. Our results establish MacRAG as an efficient, scalable solution for real-world long-context, multi-hop reasoning. Our code is available at https://github.com/Leezekun/MacRAG.

## ğŸ’¡ æˆ‘çš„æ€è€ƒ

- [ ] æœ¬æ–‡ä¸æˆ‘çš„è¯¾é¢˜å…³è”ç‚¹ï¼šè¿™ç¯‡æ–‡ç« ç»™å‡ºäº†ä¸€ä¸ªè‡ªé€‚åº”å°ºåº¦çš„ RAG æ¨¡å‹ï¼Œä¹Ÿè®¸å¯¹æˆ‘ä»¬ç›®å‰çš„å·¥ä½œæœ‰å¸®åŠ©ã€‚
- [ ] å¯å¤ç°çš„å®éªŒï¼š_________
